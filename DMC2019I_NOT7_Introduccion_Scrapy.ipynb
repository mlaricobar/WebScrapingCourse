{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMC2019I_NOT7_Introduccion_Scrapy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlaricobar/WebScrapingCourse/blob/master/DMC2019I_NOT7_Introduccion_Scrapy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhbwqHjTLY2P",
        "colab_type": "text"
      },
      "source": [
        "# Introducción a Scrapy\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/2400/0*RP3QEulh5aepQ_Ef.png)\n",
        "\n",
        "\n",
        "Scrapy es un framework de Web Scraping que permite a un desarrollador escribir código para crear un spider, que define cómo una página (o grupo de páginas) será scrapeada.\n",
        "\n",
        "Una de las características más importantes de Scrapy es que está construida sobre Twisted, una librería de red asíncrona, por lo que Scrapy está implementada de forma no bloqueante (asíncrona) permitiendo así la concurrencia de procesamiento, lo que hace que el rendimiento de scraping sea muy bueno.\n",
        "\n",
        "Nota: Cuando haces algo de forma sincrónica, esperas a que termine una tarea antes de pasar a otra tarea. Cuando haces algo de forma asíncrona, puedes pasar a otra tarea antes de que finalice.\n",
        "\n",
        "Scrapy está escrito sobre Python netamente y depende de algunos packages principales de Python:\n",
        "- lxml\n",
        "- parsel\n",
        "- w3lib\n",
        "- twisted\n",
        "- cryptography y pyOpenSSL\n",
        "\n",
        "En mi opinión, esta es una de las herramientas disponibles actualmente en Python que permite manejar tareas complejas de Web Scraping. Puede almacenar en caché las páginas web y agregar paralelismo; solo necesitamos configurar Scrapy correctamente y escribir el código de extracción."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGLIDSDD1dbU",
        "colab_type": "text"
      },
      "source": [
        "### Comparación contra BeautifulSoup\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Las 2 herramientas de Web Scraping han sido creadas para diferentes tareas. BeatifulSoup es solamente usado para analizar el código HTML y extraer los datos, por otro lado Scrapy es utilizado para la descarga del HTML, el procesamiento de los datos y su almacenamiento.\n",
        "\n",
        "Cuando comparamos BeautifulSoup vs Scrapy para descubrir cual es la mejor herramienta para nuestro proyecto, debemos considerar los siguientes factores:\n",
        "\n",
        "\n",
        "| Framework | BeautifulSoup   | Scrapy   |\n",
        "|------|------|------|\n",
        "|   Curva de Aprendizaje  | Muy fácil de aprender, amigable para los principiantes.|La curva de aprendizaje de Scrapy es mucho más pronunciada, necesita comenzar revisando un tutorial o la documentación de Scrapy, y luego trabajar duro para convertirse en un experto en Scrapy.|\n",
        "|   Ecosistema  | No hay muchos proyectos o plugins relacionados |Muchos proyectos relacionados, plugins disponibles en Github y muchas discusiones en StackOverflow pueden ayudarlo a solucionar un potencial problema.|\n",
        "|   Extensibilidad  | No es muy fácil ampliar el proyecto. |Puede desarrollar fácilmente un middleware o un pipeline para agregar funciones personalizadas, son fáciles de mantener.|\n",
        "|   Rendimiento  | Necesita importar el package multiprocessing para que se ejecute más rápido | Muy eficientes, las páginas web se pueden scrapear en poco tiempo, por otro lado, en muchos casos es necesario configurar download_delay para evitar que el spider sea eliminado (banned).|\n",
        "\n",
        "\n",
        "En resumen, si no tienes mucha experiencia en programación o el trabajo resulta ser muy simple, entonces BeautifulSoup puede ser la mejor opción. Por otro lado, si necesitamos un web scraper más potente y flexible, o si de hecho contamos con algo de experiencia en programación con Python, Scrapy definitivamente es el ganador aquí."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF_vxDb5Lp36",
        "colab_type": "text"
      },
      "source": [
        "### Instalación\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Los desarrolladores de Scrapy recomiendan instalar esta herramienta en un entorno virtual. Esta es una buena práctica para tener una versión más limpia de la herramienta; y esto le impide actualizar una dependencia de Scrapy a una versión no compatible, lo que hará que su Scraper no funcione.\n",
        "\n",
        "Revisar [virtualenvs](https://realpython.com/python-virtual-environments-a-primer/) para más detalle acerca de los entornos virtuales en python.\n",
        "\n",
        "```\n",
        "-- Instalación del package virtualenv\n",
        "pip install virtualenv\n",
        "\n",
        "-- Creación del un ambiente\n",
        "cd ~\n",
        "mkdir Virtualenvs\n",
        "cd Virtualenvs\n",
        "virtualenv scrapy_env\n",
        "\n",
        "-- Activar el ambiente\n",
        "source scrapy_env/bin/activate\n",
        "\n",
        "-- Instalación de Scrapy\n",
        "pip install Scrapy\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa_OuQwDMA8m",
        "colab_type": "code",
        "outputId": "1c083dd1-d9cd-4520-c9ca-9cb09892803c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "!pip install Scrapy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Scrapy in /usr/local/lib/python3.6/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyOpenSSL in /usr/local/lib/python3.6/dist-packages (from Scrapy) (19.0.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy) (1.20.0)\n",
            "Requirement already satisfied: cssselect>=0.9 in /usr/local/lib/python3.6/dist-packages (from Scrapy) (1.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from Scrapy) (4.2.6)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from Scrapy) (1.12.0)\n",
            "Requirement already satisfied: service-identity in /usr/local/lib/python3.6/dist-packages (from Scrapy) (18.1.0)\n",
            "Requirement already satisfied: parsel>=1.5 in /usr/local/lib/python3.6/dist-packages (from Scrapy) (1.5.1)\n",
            "Requirement already satisfied: Twisted>=13.1.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy) (19.2.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from Scrapy) (2.0.5)\n",
            "Requirement already satisfied: queuelib in /usr/local/lib/python3.6/dist-packages (from Scrapy) (1.5.0)\n",
            "Requirement already satisfied: cryptography>=2.3 in /usr/local/lib/python3.6/dist-packages (from pyOpenSSL->Scrapy) (2.6.1)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy) (0.4.5)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy) (19.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy) (0.2.5)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy) (0.7.0)\n",
            "Requirement already satisfied: PyHamcrest>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy) (1.9.0)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy) (4.6.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy) (15.1.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy) (17.5.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy) (19.0.0)\n",
            "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3->pyOpenSSL->Scrapy) (0.24.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3->pyOpenSSL->Scrapy) (1.12.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from PyHamcrest>=1.9.0->Twisted>=13.1.0->Scrapy) (41.0.1)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0->Scrapy) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->pyOpenSSL->Scrapy) (2.19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn_4jjxrJZ3Z",
        "colab_type": "text"
      },
      "source": [
        "### Construcción de mi Primer Spider\n",
        "\n",
        "En este módulo veremos como crear nuestro primer proyecto con Scrapy y nuestro mi primer spider de Scrapy. Adicionalmente, empezaremos a usar y comprender algunos comandos básicos de Scrapy.\n",
        "\n",
        "#### Comandos de Scrapy\n",
        "\n",
        "Primero, revisemos brevemente algunos comandos de Scrapy para tener una primera impresión de lo que realizan, y luego podremos aprender un poco más sobre ellos. Por ejemplo escribamos scrapy en el terminal y veamos que obtenemos.\n",
        "\n",
        "```\n",
        "$ scrapy\n",
        "Scrapy 1.6.0 - no active project\n",
        "\n",
        "Usage:\n",
        "  scrapy <command> [options] [args]\n",
        "\n",
        "Available commands:\n",
        "  bench         Run quick benchmark test\n",
        "  fetch         Fetch a URL using the Scrapy downloader\n",
        "  genspider     Generate new spider using pre-defined templates\n",
        "  runspider     Run a self-contained spider (without creating a project)\n",
        "  settings      Get settings values\n",
        "  shell         Interactive scraping console\n",
        "  startproject  Create new project\n",
        "  version       Print Scrapy version\n",
        "  view          Open URL in browser, as seen by Scrapy\n",
        "\n",
        "  [ more ]      More commands available when run from project directory\n",
        "\n",
        "Use \"scrapy <command> -h\" to see more info about a command\n",
        "```\n",
        "\n",
        "Como podemos ver, aquí hay una breve lista de comandos de Scrapy. Ahora si queremos ver a detalle algún comando en particular, simplemente usemos `scrapy <comando> -h`. En este módulo, vamos a hacer uso de los comandos `startproject` y `genspider` para empezar a crear un proyecto de Scrapy y un archivo spider. Luego veremos cómo usar los comandos `shell` y `fetch` para poder probar nuestro código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45mJT0PpMBCN",
        "colab_type": "text"
      },
      "source": [
        "#### Crear un proyecto Simple\n",
        "\n",
        "Ahora empecemos a crear un nuevo proyecto desde cero.\n",
        "\n",
        "```\n",
        "$ scrapy startproject mi_primer_spider\n",
        "```\n",
        "Una vez ejeuctado el comando nos podemos dar cuenta de que nuestro proyecto `mi_primer_spider` ha sido creado. Una vez realizado esto, podemos ver que como salida obtenemos una sugerencia de usar `genspider` para poder gener un spider de Scrapy.\n",
        "\n",
        "```\n",
        "You can start your first spider with:\n",
        "    cd mi_primer_spider\n",
        "    scrapy genspider example example.com\n",
        "```\n",
        "\n",
        "Ejecutemos la sugerencia para ver qué obtenemos.\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "├── scrapy.cfg                # Archivo de configuración de despliegue\n",
        "└── scrapy_spider             # Modulo de Proyectos, aquí podemos importar nuestro código\n",
        "    ├── __init__.py\n",
        "    ├── items.py              # Archivo de definición de los items del proyecto\n",
        "    ├── middlewares.py        # Archivos de los middlewares del proyecto\n",
        "    ├── pipelines.py          # Archivo del pipeline del proyecto\n",
        "    ├── settings.py           # Archivo de configuración del proyecto\n",
        "    └── spiders               # Directorio donde se encuentran nuestros spiders\n",
        "        ├── __init__.py\n",
        "        └── example.py        # Spider que recientemente hemos creado\n",
        "\n",
        "```\n",
        "\n",
        "Definición de Conceptos:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Spiders**: Los Spiders son clases que definen cómo un determinado sitio (o un grupo de sitios) será scrapeado, que comprende el cómo realizar el crawling (es decir, seguir los enlaces) y cómo extraer los datos estructurados de sus páginas (Items). En otras palabras, los spiders son los lugares donde definimos el comportamiento personalizado para scrapear y analizar las páginas de un sitio en particular (o, en algunos casos, un grupo de sitios).\n",
        "2.   **Items**: Definición de una estructura que contemplará los datos extraídos de la página así como el tratamiento adhoc de los mismos.\n",
        "3.   **pipelines**: Son clases definidas en Scrapy que permiten procesar los datos extraídos por el Scraping a través de varios componentes que se ejecutan de forma secuencial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKKPhku3SV8o",
        "colab_type": "text"
      },
      "source": [
        "#### Arquitectura de Scrapy\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*mYXqmVMrbvyp8ZSSkx9lKQ.png)\n",
        "\n",
        "\n",
        "El flujo de datos en Scrapy está controlado por el motor de ejecución y es de la siguiente forma:\n",
        "\n",
        "\n",
        "\n",
        "1.   El motor obtiene las solicitudes iniciales desde el Spider para realizar el web crawling.\n",
        "2.   El motor programa las solicitudes en el Scheduler y solicita las siguientes solicitudes para hacer crawling.\n",
        "3.   El Scheduler devuelve las siguientes Solicitudes al Motor.\n",
        "4.   El motor envía las solicitudes al Donwloade, pasando a través de los Donwloader Middlewares.\n",
        "5.   Una vez que la página termina de descargar, el Downloader genera una Respuesta (con esa página) y la envía al Motor, pasando a través de los Donwloader Middlewares.\n",
        "6.   El motor recibe la respuesta del Downloader y la envía al Spider para su procesamiento, pasando a través del Spider Middleware.\n",
        "7.   El Spider procesa la Respuesta y devuelve los elementos extraídos y las nuevas Solicitudes (para seguir) al Motor, pasando a través del Spider Middleware (consulte process_spider_output ()).\n",
        "8.   El motor envía los elementos procesados a los , luego envía las solicitudes procesadas al programador y solicita las posibles siguientes solicitudes de rastreo.\n",
        "El proceso se repite (desde el paso 1) hasta que no haya más solicitudes del Programador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbBgt_JSSdyk",
        "colab_type": "text"
      },
      "source": [
        "#### Construcción del Spider\n",
        "Si revisamos nuestro archivo example.py veremos lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class ExampleSpider(scrapy.Spider):\n",
        "    name = 'example'\n",
        "    allowed_domains = ['example.com']\n",
        "    start_urls = ['http://example.com/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        pass\n",
        "\n",
        "```\n",
        "\n",
        "Como podemos ver, el archivo spider contiene algunos parámetros, por ejemplo:\n",
        "\n",
        "\n",
        "\n",
        "1.   **name**: Identifica al Spider. Dentro de un proyecto este debe ser único.\n",
        "2.   **start_urls**: La lista de URLs semillas, que el spider empezará a realizar web crawling.\n",
        "3.   **allowed_domains**: Esta configuración es útil para realizar web crawling de una forma más amplia, es decir si el dominio de la URL no está en esta configuración, entonces esa URL será ignorada.\n",
        "4.   **parse**: Es un método que será llamado para manejar el reponse obtenido por cada petición realizado.\n",
        "\n",
        "\n",
        "Como ejemplo podemos, modificar nuestro archivo example.py para poder scrapear algunos datos de [la página del catálogo de celulares de Linio](https://www.linio.com.pe/c/celulares-y-tablets).\n",
        "\n",
        "Es decir podemos cambiarlo de la siguiente forma:\n",
        "\n",
        "```python\n",
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class LinioSpider(scrapy.Spider):\n",
        "    name = 'linio_spider'\n",
        "    allowed_domains = ['linio.com.pe']\n",
        "    start_urls = ['https://www.linio.com.pe/c/celulares-y-tablets/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        product_titles = response.xpath(\"//div[@id='catalogue-product-container']//div[@class='detail-container']//span[@class='title-section']/text()\").extract()\n",
        "    \t  yield {'product_titles': product_titles}\n",
        "\n",
        "```\n",
        "Validemos que efectivamente el xpath especificado hace referencia a los títulos de los productos. Una vez que ya tenemos nuestro Spider, podemos ejecutarlo de la siguiente manera:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "$ scrapy crawl linio_spider\n",
        "```\n",
        "\n",
        "Deberíamos ver el resultado del diccionario con la lista de títulos de los productos de la página Linio.\n",
        "\n",
        "Nota: En caso de obtener respuesta 404 a la petición realizada, descomentar el parámetro `USER_AGENT` en el archivo de `settings.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxUgOElhcvMc",
        "colab_type": "text"
      },
      "source": [
        "Como ejercicio haremos lo siguiente:\n",
        "\n",
        "*   Definir otros datos para extraer como la url de la imagen,  el precio y el valor de descuento.\n",
        "*   Definir la lista de urls a los que se extraerá la información de todos los productos del catálogo de celulares de Linio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWC1NAYaohxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG4H-8YMkQPM",
        "colab_type": "text"
      },
      "source": [
        "En esta actividad, hemos creado con éxito un Proyecto de Scrapy y un Spider usando algunos comandos de Scrapy. Adicionalmente, hemos desarrollado un spider que nos permita realizar Web Scraping a la página de Linio. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68EQRcelsgv",
        "colab_type": "text"
      },
      "source": [
        "### Comandos de Scrapy Shell\n",
        "\n",
        "Scrapy shell nos permite extraer datos y utilizarlo como una herramienta de desarrollo del spider. Se recomienda tener instalado `IPython` antes de usarlo. Para poder usarlo debemos utilizar el comando `scrapy shell`,  luego veremos lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "2019-05-22 20:18:44 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapy_spider)\n",
        "[s] Available Scrapy objects:\n",
        "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
        "[s]   crawler    <scrapy.crawler.Crawler object at 0x10b5d84e0>\n",
        "[s]   item       {}\n",
        "[s]   settings   <scrapy.settings.Settings object at 0x10a0aff28>\n",
        "[s] Useful shortcuts:\n",
        "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
        "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n",
        "[s]   shelp()           Shell help (print this help)\n",
        "[s]   view(response)    View response in a browser\n",
        "```\n",
        "\n",
        "En la mayoría de los casos, la salida es un poco más larga porque otra información, como el middleware y la extensión, también se muestra en el terminal, que se puede ignorar por el momento. En la salida anterior, como puedes ver, hay algunos objetos de Scrapy disponibles en este shell de IPython.\n",
        "\n",
        "Usaremos el comando fetch para poder realizar Web Crawling a la URL definida por nosotros, por ejemplo continuaremos utilizando la página del catálogo de celulares de Linio.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "fetch('https://www.linio.com.pe/c/celulares-y-tablets/')\n",
        "```\n",
        "\n",
        "Al revisar el response de la petición, si obtenemos lo siguiente:\n",
        "\n",
        "```\n",
        "DEBUG: Crawled (404) <GET https://www.linio.com.pe/c/celulares-y-tablets/> (referer: None\n",
        "```\n",
        "\n",
        "nos podemos dar cuenta de que la página bloquea la solicitud que estamos realizando con el framework de Scrapy, y es debido a que el user agent por defecto que utiliza Scrapy es **Scrapy/1.6.0 (+https://scrapy.org)**. Esto lo podemos ver al pintar la variable `settings`.\n",
        "\n",
        "Para solucionar esto, debemos definir el user agent tal como en el spider creado anteriormente `mi_primer_spider (+http://www.yourdomain.com)` o `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36`.\n",
        "\n",
        "Para definir el user agent como header en la petición ejecutar el siguiente código:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "url = 'https://www.linio.com.pe/c/celulares-y-tablets/'\n",
        "request = scrapy.Request(url, headers={'User-Agent': 'mi_primer_spider (+http://www.yourdomain.com)'})\n",
        "fetch(request)\n",
        "```\n",
        "\n",
        "Luego de haber realizado la petición podríamos empezar a revisar el detalle de la página usando el objeto `response`.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "reponse.body           #Pintar todo el contenido de la página\n",
        "\n",
        "response.xpath(\"//div[@id='catalogue-product-container'])\")      #Selector que contiene el elemento con id catalogue-product-container donde se encuentran los datos de los productos.\n",
        "\n",
        "response.xpath(\"//div[@id='catalogue-product-container'])\").extract()      #Pintar la lista de los elementos que cumplan con el xpath especificado\n",
        "\n",
        "response.xpath(\"//div[@id='catalogue-product-container'])\").extract_first()      #Pintar el primer elemento que cumpla con el xpath especificado\n",
        "\n",
        "product_titles = response.xpath(\"//div[@id='catalogue-product-container']//div[@class='detail-container']//span[@class='title-section']/text()\").extract()           #Lista de titulos de productos\n",
        "\n",
        "img_urls = response.xpath(\"//div[@id='catalogue-product-container']//div[@class='image-container']//img/@data-lazy\").extract()           #Lista de urls de las imágenes de los productos\n",
        "\n",
        "product_prices = response.xpath(\"//div[@id='catalogue-product-container']//div[@class='detail-container']//span[@class='price-main']/text()\").extract()           #Lista de los precios de los productos\n",
        "\n",
        "product_discounts = response.xpath(\"//div[@id='catalogue-product-container']//div[@class='detail-container']//span[@class='discount']/text()\").extract()          #Lista de los valores de descuentos de los productos\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "En esta sección hemos visto el uso de Scrapy Shell que nos permitirá desarrollar y hacer pruebas rápidamente sobre los Spiders que queremos definir.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yRsZ4FtU0NV",
        "colab_type": "text"
      },
      "source": [
        "Otra forma de como extraer los valores de los productos es de la siguiente forma:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "from scrapy.selector import Selector\n",
        "\n",
        "class ExampleSpider(scrapy.Spider):\n",
        "    name = 'linio_spider'\n",
        "    allowed_domains = ['linio.com.pe']\n",
        "    start_urls = ['https://www.linio.com.pe/c/celulares-y-tablets/']\n",
        "    #start_urls = [\"https://www.linio.com.pe/c/celulares-y-tablets?page={0}\".format(i) for i in range(1,18)]\n",
        "\n",
        "    def parse(self, response):\n",
        "    \tproduct_list = response.xpath(\"//div[@id='catalogue-product-container']//div[@class='catalogue-product row']\")#.extract()\n",
        "\n",
        "    \tfor product_selector in product_list:\n",
        "    \t\t#product_selector = Selector(text=product)\n",
        "    \t\tyield {\n",
        "    \t\t\"title\": product_selector.xpath(\".//span[@class='title-section']/text()\").extract_first(),\n",
        "    \t\t\"img_url\": product_selector.xpath(\".//div[@class='image-container']//img/@data-lazy\").extract_first(),\n",
        "    \t\t\"product_price\": product_selector.xpath(\".//div[@class='detail-container']//span[@class='price-main']/text()\").extract_first(),\n",
        "    \t\t\"product_discount\": product_selector.xpath(\".//div[@class='detail-container']//span[@class='discount']/text()\").extract_first()\n",
        "    \t\t}\n",
        "```\n",
        "\n",
        "Nota: Si se han dado cuenta en el xpath de cada campo hemos añadido el . al inicio para que realice la busqueda a nivel de selector del loop, sino lo hará a nivel del documento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euJb-ChvZU9p",
        "colab_type": "text"
      },
      "source": [
        "### Paginación\n",
        "\n",
        "Una forma de realizar web crawling es iterar sobre una lista de urls, en caso de que se de la casuísitca de no tener un patrón deberíamos iterar sobre un paginado. En esta sección veremos como implementarlo en la página de Linio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEIosxg-VCY-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "next_page = response.xpath(\"//nav[@class='pagination-container']//a[@class='page-link page-link-icon']/@href\").extract()[-2]  #Obtiene la url del paginado\n",
        "    \tif next_page is not None:\n",
        "    \t\tnext_page_link= response.urljoin(next_page)\n",
        "    \t\tyield scrapy.Request(url=next_page_link, callback=self.parse)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLIK-RNDfqix",
        "colab_type": "text"
      },
      "source": [
        "Ejecutar el Spider\n",
        "\n",
        "Para ejecutar el spider y guardar los datos debemos ejecutar lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "scrapy crawl linio_spider -o data.json\n",
        "```\n",
        "\n",
        "Si hasta el momento revisamos los datos obtenidos nos podemos dar cuenta de que algunos necesitan limpieza por ejemplo el precio del producto.\n",
        "\n",
        "![alt text](https://github.com/mikolarico/course-images/blob/master/Screen%20Shot%202019-05-24%20at%2015.20.15.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ol46FolmlYk",
        "colab_type": "text"
      },
      "source": [
        "Un enfoque de limpieza sería el cargar nuestros datos a un dataframe y luego aplicarle operaciones de limpieza. Revisar el otro notebook de Exploración.\n",
        "\n",
        "\n",
        "Sin embargo, Scrapy proporciona una forma de cómo poder realizar la limpieza de los datos extraídos a través del uso de los `Items`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TDaAwH3T0Wk",
        "colab_type": "text"
      },
      "source": [
        "### Items\n",
        "\n",
        "\n",
        "Los Spiders de Scrapy pueden devolver los datos extraídos como diccionarios de Python. Si bien son convenientes y familiares, los diccionarios de Python carecen de estructura: es fácil de obtener un error por el tipo en un nombre de campo o de obtener datos inconsistentes, especialmente en un proyecto grande con muchos Spiders.\n",
        "\n",
        "\n",
        "Para definir un formato de datos comun Scrapy proporciona la clase `Item`. Los objetos de la clase `Item` son contenedores simples usados para coleccionar los datos scrapeados. Ellos proporcionan un API (casi con el formato de diccionario) con una sintaxis conveniente para declarar sus campos o atributos.\n",
        "\n",
        "En el archivo items.py definir lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Define here the models for your scraped items\n",
        "#\n",
        "# See documentation in:\n",
        "# https://doc.scrapy.org/en/latest/topics/items.html\n",
        "\n",
        "import scrapy\n",
        "import re\n",
        "from scrapy.loader.processors import MapCompose, TakeFirst\n",
        "\n",
        "\n",
        "def remove_whitespace(value):\n",
        "\treturn value.strip()\n",
        "\n",
        "def set_prefix_url(value):\n",
        "\treturn 'https' + value\n",
        "\n",
        "def clean_price_value(value):\n",
        "\treturn float(re.search(r'S/\\s(.+)', value).group(1).replace(\",\", \"\"))\n",
        "\n",
        "def clean_discount_value(value):\n",
        "\treturn int(re.search(r'\\d+', value).group())\n",
        "\n",
        "\n",
        "\n",
        "class MiPrimerSpiderItem(scrapy.Item):\n",
        "    # define the fields for your item here like:\n",
        "    # name = scrapy.Field()\n",
        "    title = scrapy.Field(\n",
        "    \tinput_processor = MapCompose(remove_whitespace, set_prefix_url), \n",
        "    \toutput_processor=TakeFirst()\n",
        "    \t)\n",
        "\n",
        "    img_url = scrapy.Field(output_processor=TakeFirst())\n",
        "    product_price = scrapy.Field(input_processor=MapCompose(remove_whitespace, clean_price_value), output_processor=TakeFirst())\n",
        "    product_discount = scrapy.Field(input_processor=MapCompose(remove_whitespace, clean_discount_value), output_processor=TakeFirst())\n",
        "    url_pagination = scrapy.Field(output_processor=TakeFirst())\n",
        "\n",
        "```\n",
        "\n",
        "y en el archivo example.py:\n",
        "\n",
        "```python\n",
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "from mi_primer_spider.items import MiPrimerSpiderItem\n",
        "from scrapy.loader import ItemLoader\n",
        "\n",
        "class ExampleSpider(scrapy.Spider):\n",
        "    name = 'linio_spider'\n",
        "    allowed_domains = ['linio.com.pe']\n",
        "    start_urls = ['https://www.linio.com.pe/c/celulares-y-tablets/']\n",
        "    #start_urls = [\"https://www.linio.com.pe/c/celulares-y-tablets?page={0}\".format(i) for i in range(1,18)]\n",
        "\n",
        "    def parse(self, response):\n",
        "\n",
        "\n",
        "    \tproduct_list = response.xpath(\"//div[@id='catalogue-product-container']//div[@class='catalogue-product row']\")#.extract()\n",
        "\n",
        "    \tfor product_selector in product_list:\n",
        "    \t\t#product_selector = Selector(text=product)\n",
        "    \t\tl = ItemLoader(item=MiPrimerSpiderItem(), selector=product_selector)\n",
        "    \t\tl.add_xpath(\"title\", \".//span[@class='title-section']/text()\")\n",
        "    \t\tl.add_xpath(\"img_url\", \".//div[@class='image-container']//img/@data-lazy\")\n",
        "    \t\tl.add_xpath(\"product_price\", \".//div[@class='detail-container']//span[@class='price-main']/text()\")\n",
        "    \t\tl.add_xpath(\"product_discount\", \".//div[@class='detail-container']//span[@class='discount']/text()\")\n",
        "    \t\tl.add_value(\"url_pagination\", response.request.url)\n",
        "    \t\tyield l.load_item()\n",
        "\n",
        "    \tnext_page = response.xpath(\"//nav[@class='pagination-container']//a[@class='page-link page-link-icon']/@href\").extract()[-2]\n",
        "    \tif next_page is not None:\n",
        "    \t\tnext_page_link= response.urljoin(next_page)\n",
        "    \t\tyield scrapy.Request(url=next_page_link, callback=self.parse)\n",
        "\n",
        "    \t\t#data_list.append(data_row)\n",
        "    \t#yield {\"data\": data_list}\n",
        "      \n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNwRBowWUyns",
        "colab_type": "text"
      },
      "source": [
        "### Ejecución en Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC5hwmjmUyCf",
        "colab_type": "code",
        "outputId": "050188c7-7343-45e4-df6f-998244d3b7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "!scrapy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scrapy 1.6.0 - no active project\n",
            "\n",
            "Usage:\n",
            "  scrapy <command> [options] [args]\n",
            "\n",
            "Available commands:\n",
            "  bench         Run quick benchmark test\n",
            "  fetch         Fetch a URL using the Scrapy downloader\n",
            "  genspider     Generate new spider using pre-defined templates\n",
            "  runspider     Run a self-contained spider (without creating a project)\n",
            "  settings      Get settings values\n",
            "  shell         Interactive scraping console\n",
            "  startproject  Create new project\n",
            "  version       Print Scrapy version\n",
            "  view          Open URL in browser, as seen by Scrapy\n",
            "\n",
            "  [ more ]      More commands available when run from project directory\n",
            "\n",
            "Use \"scrapy <command> -h\" to see more info about a command\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuxmrZgyUwVe",
        "colab_type": "code",
        "outputId": "2d2ffe03-4e3d-4813-d2dc-eba005a62d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMkdStBLU8U5",
        "colab_type": "code",
        "outputId": "5e605b0a-1956-40a5-f0c3-d1317cf49bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!ls -l /gdrive/'My Drive'/Work/DMC/'01. Web Scraping'/'Clase 04'/mi_primer_spider"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 35\n",
            "-rw------- 1 root root 31195 May 30 20:06 DMC2019I_NOT7_Pipeline_Limpieza.ipynb\n",
            "drwx------ 4 root root  4096 May 30 20:06 mi_primer_spider\n",
            "-rw------- 1 root root   275 May 30 20:06 scrapy.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fir-WtZVR2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /gdrive/'My Drive'/Work/DMC/'01. Web Scraping'/'Clase 04'/mi_primer_spider mi_primer_spider"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbbzlZuwVbWy",
        "colab_type": "code",
        "outputId": "5f308cef-d8fc-4fbb-a36f-2a8868afe521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/mi_primer_spider"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mi_primer_spider\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNMryKg6VbiH",
        "colab_type": "code",
        "outputId": "46dbada0-c5cf-44fb-88c2-d205761b08a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 40\n",
            "-rw------- 1 root root 31195 May 30 20:12 DMC2019I_NOT7_Pipeline_Limpieza.ipynb\n",
            "drwx------ 4 root root  4096 May 30 20:12 mi_primer_spider\n",
            "-rw------- 1 root root   275 May 30 20:12 scrapy.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-qpjsXKVbHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy crawl linio_spider_v2 -o data_v2.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDH1hKHnmzQU",
        "colab_type": "code",
        "outputId": "d8c44880-cf87-4538-9428-5c008730cbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 5392\n",
            "-rw-r--r-- 1 root root 5480272 May 30 20:19 data_v2.json\n",
            "-rw------- 1 root root   31195 May 30 20:12 DMC2019I_NOT7_Pipeline_Limpieza.ipynb\n",
            "drwx------ 4 root root    4096 May 30 20:12 mi_primer_spider\n",
            "-rw------- 1 root root     275 May 30 20:12 scrapy.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBPB4q5Km2ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daGQgXRTm4fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_json(\"data_v2.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhFdxpBYm-Du",
        "colab_type": "code",
        "outputId": "afbf7de6-f4ef-4362-ccbe-dd0255100935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13892, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJdu1ZtSnAdW",
        "colab_type": "code",
        "outputId": "2424ae02-0c96-45a3-ffa6-52756707f10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>availability</th>\n",
              "      <th>brand</th>\n",
              "      <th>category</th>\n",
              "      <th>image</th>\n",
              "      <th>model</th>\n",
              "      <th>price</th>\n",
              "      <th>priceCurrency</th>\n",
              "      <th>sku</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>Ultra HD / 4K TV</td>\n",
              "      <td>//i.linio.com/p/2640370e05d49b2a67c1b3f1389592...</td>\n",
              "      <td>UN58NU7100G</td>\n",
              "      <td>1599.00</td>\n",
              "      <td>PEN</td>\n",
              "      <td>SA026EL0AM8GELPE</td>\n",
              "      <td>Tv led Smart Samsung 58\" UN58NU7100G Ultra HD ...</td>\n",
              "      <td>/p/tv-led-smart-samsung-58-un58nu7100g-ultra-h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Apple</td>\n",
              "      <td>Desbloqueados</td>\n",
              "      <td>//i.linio.com/p/a5f87f2fc4d52a078e17a100425d0c...</td>\n",
              "      <td>iPhone 8 Plus</td>\n",
              "      <td>1819.00</td>\n",
              "      <td>PEN</td>\n",
              "      <td>AP032EL043GG6LPE</td>\n",
              "      <td>Apple iPhone 8 Plus 64GB - Space Gray</td>\n",
              "      <td>/p/apple-iphone-8-plus-64gb-space-gray-ymcw0x</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>National</td>\n",
              "      <td>Cocinas</td>\n",
              "      <td>//i.linio.com/p/533c757f0144d1efb31564baacacf2...</td>\n",
              "      <td>Pro 2400 W 01 Hornilla</td>\n",
              "      <td>89.99</td>\n",
              "      <td>PEN</td>\n",
              "      <td>NA979HL0MHEP2LPE</td>\n",
              "      <td>Cocina De Induccion LED Digital National Pro 2...</td>\n",
              "      <td>/p/cocina-de-induccion-led-digital-national-pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Go Pro</td>\n",
              "      <td>Cámaras Acuáticas</td>\n",
              "      <td>//i.linio.com/p/31379c2ce463b3da6be70014121f2a...</td>\n",
              "      <td>CHDHX-502</td>\n",
              "      <td>715.00</td>\n",
              "      <td>PEN</td>\n",
              "      <td>GO903EL0F6XDSLPE</td>\n",
              "      <td>Go Pro Hero 5, Negro, 1x12 Megapixel CMOS Sens...</td>\n",
              "      <td>/p/go-pro-hero-5-negro-1x12-megapixel-cmos-sen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>CL IMPORT</td>\n",
              "      <td>Cepillos y peines</td>\n",
              "      <td>//i.linio.com/p/9d536da8100c2d1dd8e88a9730368e...</td>\n",
              "      <td></td>\n",
              "      <td>28.00</td>\n",
              "      <td>PEN</td>\n",
              "      <td>CL708HB1A71XSLPE</td>\n",
              "      <td>Ultima Version Cepillo Alisador Straight Artif...</td>\n",
              "      <td>/p/ultima-version-cepillo-alisador-straight-ar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   availability  ...                                                url\n",
              "0           NaN  ...  /p/tv-led-smart-samsung-58-un58nu7100g-ultra-h...\n",
              "1           NaN  ...      /p/apple-iphone-8-plus-64gb-space-gray-ymcw0x\n",
              "2           NaN  ...  /p/cocina-de-induccion-led-digital-national-pr...\n",
              "3           NaN  ...  /p/go-pro-hero-5-negro-1x12-megapixel-cmos-sen...\n",
              "4           NaN  ...  /p/ultima-version-cepillo-alisador-straight-ar...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcnpkGNRnNDo",
        "colab_type": "code",
        "outputId": "efb0a13e-b213-426f-dac0-9588d719dbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df[\"url\"].nunique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13892"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS4yqhfUnM66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp data_v2.json /gdrive/'My Drive'/Work/DMC/'01. Web Scraping'/'Clase 04'/mi_primer_spider/data_v2.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ODv1MeDUUN5",
        "colab_type": "text"
      },
      "source": [
        "### Item Pipeline\n",
        "Despues que un Item haya sido *scrapeado* por un Spider, será enviado a un Pipeline para que sea procesado por varios componentes. Cada Pipeline es una clase de Python que implementa un método simple. El componente puede realizar una acción sobre el Item, modificarlo, eliminarlo, o enviarlo a otro componente.\n",
        "\n",
        "El Pipeline del Item puede ser usado para validar los datos scrapeados, revisar el duplicado de datos, o insertar los datos en bases de datos tales como MySQL, PostgreSQL, CosmosDB o MongoDB.\n",
        "\n",
        "En este pequeño módulo, construiremos un pipeline que permita almacenar los datos scrapeados en una Base de Datos como Azure Cosmos DB.\n",
        "\n",
        "\n",
        "**En el archivo Settings.py:**\n",
        "En el archivo de configuraciones debemos definir las variables de cadenas de conexión que utilizaremos dentro del Pipeline.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# Adhoc settings\n",
        "ENDPOINT = \"##### < PASTE YOUR ENDPOINT HERE > #####\"\n",
        "MASTERKEY = \"##### < PASTE YOUR MASTER KEY HERE > #####\"\n",
        "\n",
        "COLLECTION_LINK =  \"##### < PASTE YOUR COLLECTION KEY HERE > #####\"\n",
        "\n",
        "\n",
        "ITEM_PIPELINES = {\n",
        "    'linio_project_v2.pipelines.ProductPipeline': 300,\n",
        "}\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**En el archivo Pipeline.py**: Aquí debemos definir la clase del Pipeline así como la lógica que se implementará por cada Item. \n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Define your item pipelines here\n",
        "#\n",
        "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
        "# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "\n",
        "from pydocumentdb import documents\n",
        "from pydocumentdb import document_client\n",
        "\n",
        "from scrapy.utils.project import get_project_settings as get_set\n",
        "\n",
        "\n",
        "class ProductPipeline(object):\n",
        "\tdef __init__(self):\n",
        "\t\t\"\"\"\n",
        "\t\tInicia el cliente para conectarnos a la BD de Cosmos.\n",
        "\t\t\"\"\"\n",
        "\t\tconnectionPolicy = documents.ConnectionPolicy()\n",
        "\t\tconnectionPolicy.EnableEndpointDiscovery\n",
        "\t\tconnectionPolicy.PreferredLocations = [\"East US 2\"]\n",
        "\n",
        "\t\tself.client = document_client.DocumentClient(get_set().get(\"ENDPOINT\"), {'masterKey': get_set().get(\"MASTERKEY\")}, connectionPolicy)\n",
        "\n",
        "\tdef process_item(self, item, spider):\n",
        "\t\td = dict(item)\n",
        "\t\ttry:\n",
        "\t\t\tself.client.CreateDocument(get_set().get(\"COLLECTION_LINK\"), d)\n",
        "\t\texcept:\n",
        "\t\t\traise\n",
        "\n",
        "\t\treturn item\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Al ejecutar el spider, se valida que efectivamente los registros se han ejecutado:\n",
        "\n",
        "![alt text](https://github.com/mikolarico/course-images/blob/master/product_in_cosmos.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ld2SfthAdG",
        "colab_type": "text"
      },
      "source": [
        "### Despliegue en Producción\n",
        "\n",
        "Existen diferentes formas de cómo desplegar los spiders de Scrapy que hemos desarrollado para su ejecución periódica o regular. Por lo general ejecutar los spiders de Scrapy en una máquina local es muy conveniente para la etapa de desarrollo, pero no tanto cuando se necesita ejecutar Spiders de larga ejecución o migrar Spiders para que se ejecuten en ambientes de producción continuamente. Aquí es donde entran las soluciones de despliegue de Scrapy.\n",
        "\n",
        "#### Scrapy Cloud\n",
        "\n",
        "Es un servicio en nube y proporcionado por ScrapingHub, la compañía creadora de Scrapy. Con Scrapy Cloud nos evitamos la necesidad de configurar y monitorear servidores ya que proporciona una interfaz de usuario agradable para administrar los Spiders y revisar los ítems, registros y estadísticas resultados del Scraping.\n",
        "\n",
        "Para implementar los Spiders en Scrapy Cloud, podemos usar la herramienta de línea de comandos **shub**. Para ello necesitamos instalarla con el siguiente comando:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pip install shub\n",
        "```\n",
        "\n",
        "El siguiente paso es desplegar nuestro proyecto de Scrapy en Scrapy Cloud. Para ello, necesitamos el **API key** y el **numeric ID** del proyecto. Podemos encontrar esos dos datos en la página de **Code & Deploys**. Una vez que tengamos esos datos a la mano, debemos ejecutar el siguiente comando\n",
        "\n",
        "```\n",
        "shub login\n",
        "```\n",
        "Con este comando guardamos nuestro **API key** en un archivo local (~/scrapinghub.yml). Si queremos eliminarlo usemos el comando:\n",
        "\n",
        "```\n",
        "shub logout\n",
        "```\n",
        "\n",
        "Una vez logueados, ubíquemonos en nuestro proyecto y ejecutemos el siguiente comando para desplegar nuestro proyecto a Scrapy Cloud:\n",
        "\n",
        "\n",
        "```\n",
        "shub deploy\n",
        "```\n",
        "\n",
        "Una vez que realizamos el despliegue, podemos ejecutar el spider por medio del siguiente comando:\n",
        "\n",
        "\n",
        "```\n",
        "shub schedule linio_spider\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NRkLvYRWrZa",
        "colab_type": "text"
      },
      "source": [
        "### Integración con Splash\n",
        "\n",
        "**Instalación de Splash:**\n",
        "\n",
        "La recomendación de instalar **Splash** es por medio de Docker. Referencias:\n",
        "\n",
        "*   [Instalación y Configuración de docker](https://docs.docker.com/toolbox/toolbox_install_windows/)\n",
        "*   [Configuración de Splash](https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzCzlxXR5ZbN",
        "colab_type": "text"
      },
      "source": [
        "### Integración con Selenium\n",
        "\n",
        "Para poder realizar la integración de Selenium con Scrapy, debemos utilizar el código de Selenium en el Downloader Middleware del Proyecto. \n",
        "\n",
        "Debemos recordar que los métodos de esta clase se llaman siempre que Scrapy realiza una solicitud. Modifica la solicitud / respuesta de alguna manera y la devuelve a Scrapy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_LWTR8s17GQ",
        "colab_type": "text"
      },
      "source": [
        "### Notas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgWswYeFhR9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "# importar la librería BeautifulSoup\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqnZHeE8hR7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = requests.get('https://www.linio.com.pe/c/celulares-y-tablets/')\n",
        "soup = BeautifulSoup(markup=response.content, features='html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgiOMoIzhYH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup.find(name=\"div\", attrs={\"id\": \"catalogue-product-container\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff6mocNaXIJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -U 'mi_primer_spider (+http://www.yourdomain.com)' https://www.linio.com.pe/c/celulares-y-tablets/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkufPmIbXavD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -U 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36' https://www.linio.com.pe/c/celulares-y-tablets/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}